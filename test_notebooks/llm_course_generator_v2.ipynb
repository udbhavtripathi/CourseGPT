{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install OpenAI\n",
    "# pip install langchain\n",
    "# pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# import streamlit as st\n",
    "# from PyPDF2 import PdfReader\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "# from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRIAL 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module --> Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def course_maker(topic_name):\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "\n",
    "    ################# chain1: Topic name(only 5 for now)\n",
    "\n",
    "    topic_prompt = PromptTemplate(\n",
    "        input_variables=[\"topic\"],\n",
    "        template=\"Write 3 module name for course in {topic}?\",\n",
    "    )\n",
    "\n",
    "    topic_chain = LLMChain(llm=llm, prompt=topic_prompt)\n",
    "\n",
    "    topic_output =  topic_chain.predict(topic=topic_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #####################################  MODULE:1  #############################################\n",
    "\n",
    "    # split the string into three separate strings\n",
    "    point1, point2, point3 = topic_output.split('\\n')\n",
    "\n",
    "    \n",
    "    ################# chain2: module1\n",
    "\n",
    "\n",
    "    module1_prompt = PromptTemplate(\n",
    "        input_variables=[\"module1\"],\n",
    "        template=\"Write a detailed description for the topic {module1}\",\n",
    "    )\n",
    "\n",
    "    module1_chain = LLMChain(llm=llm, prompt=module1_prompt)\n",
    "\n",
    "    module1_output =  module1_chain.predict(module1=point1)\n",
    "\n",
    "\n",
    "    ######################################  MODULE:2   #################################################\n",
    "    \n",
    "\n",
    "\n",
    "    ################# chain3: module 2\n",
    "\n",
    "    module2_prompt = PromptTemplate(\n",
    "        input_variables=[\"module2\"],\n",
    "        template=\"Write a detailed description for the topic {module2}\",\n",
    "    )\n",
    "\n",
    "    module2_chain = LLMChain(llm=llm, prompt=module2_prompt)\n",
    "\n",
    "    module2_output =  module2_chain.predict(module2=point2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ######################################  MODULE:3   #################################################\n",
    "\n",
    "\n",
    "    ################# chain4: module 3\n",
    "\n",
    "    module3_prompt = PromptTemplate(\n",
    "        input_variables=[\"module3\"],\n",
    "        template=\"Write a detailed description for the topic {module3}\",\n",
    "    )\n",
    "\n",
    "    module3_chain = LLMChain(llm=llm, prompt=module3_prompt)\n",
    "\n",
    "    module3_output =  module3_chain.predict(module3=point3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return topic_output,module1_output,module2_output,module3_output,point1,point2,point3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Lunar Geology and Mineralogy\n",
      "2. Living and Working on the Moon\n",
      "3. Lunar Science and Exploration Techniques\n"
     ]
    }
   ],
   "source": [
    "topic_output,module1_output,module2_output,module3_output,point1,point2,point3 = course_maker(topic_name='moon')\n",
    "\n",
    "print(topic_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module1_output\n",
      "Lunar geology and mineralogy are two complementary fields of study that focus on understanding the geological and mineralogical processes that have shaped the Moon. The Moon, being our closest celestial neighbor, has been a subject of fascination for astronomers, space scientists, and geologists alike. In the past decades, several space missions, including Apollo, have been sent to the Moon to gather extensive data on its geology and mineralogy. The study of lunar geology and mineralogy has revealed fascinating insights into the origin, composition, and evolution of the Moon.\n",
      "\n",
      "Lunar geology is concerned with the study of the Moon's surface features, including its craters, mountains, valleys, and plains. Through various observations and experiments, geologists have established that the Moon is primarily composed of four main layers: the crust, mantle, outer core, and inner core. The crust, which is the uppermost layer, is made up of basaltic rocks, while the mantle is composed of olivine and pyroxene. The outer and inner cores, on the other hand, are believed to be rich in iron and nickel.\n",
      "\n",
      "One of the most striking features of the Moon's surface is its craters. These are formed by meteoroid impacts, which have been occurring for billions of years. The Moon's lack of an atmosphere, combined with its weak gravitational pull, makes it vulnerable to frequent impacts. Many of the Moon's craters are still visible, with some being over 100 kilometers in diameter.\n",
      "\n",
      "Another significant area of study in lunar geology is the Moon's volcanic activity. The lunar surface is dotted with hundreds of volcanic features, including dark patches known as maria. These patches are formed by cooled lava flows that originated from volcanic eruptions. The Moon's volcanic activity is believed to have been more active in the past, with the last known eruptions occurring over a billion years ago.\n",
      "\n",
      "Lunar mineralogy, on the other hand, is concerned with the study of the Moon's mineral composition. The Moon has a unique mineral composition that is significantly different from that of the Earth. Some of the Moon's most abundant minerals include plagioclase feldspar, pyroxene, olivine, and ilmenite. These minerals are distributed unevenly across the Moon's surface, with some regions being richer in certain minerals than others.\n",
      "\n",
      "Lunar mineralogy has also revealed the presence of some rare and valuable minerals on the Moon, including helium-3, which is a potential fuel source for nuclear fusion reactors. The Moon is also believed to have deposits of iron, titanium, and other metals that could be used in future space exploration missions.\n",
      "\n",
      "In conclusion, lunar geology and mineralogy are essential fields of study that continue to reveal fascinating insights into the origin, composition, and evolution of the Moon. Through the use of advanced technology and space exploration missions, scientists are continually uncovering new information about our celestial neighbor's geology and mineralogy. The knowledge gained from these studies could be crucial in the future as we explore space and look for ways to sustain our planet's resources.\n",
      " \n",
      "module2_output\n",
      "Living and working on the Moon, also known as lunar colonization, is a concept that has been explored in science fiction for years. However, with recent advancements in technology and space exploration, the idea of humans permanently residing on the Moon is no longer a far-fetched idea. In fact, several countries and private companies are actively working towards making lunar colonization a reality.\n",
      "\n",
      "One of the primary reasons for lunar colonization is to establish a human presence beyond Earth. This would allow us to expand our knowledge and understanding of our universe, as well as provide a stepping stone to further space exploration. In addition, the Moon could potentially serve as a testing ground for technologies and procedures that could be used in future space missions, such as to Mars.\n",
      "\n",
      "Living on the Moon would present a host of challenges that would need to be overcome. The Moon has no atmosphere, so humans would need to be protected from the harsh radiation and extreme temperatures. They would also need a steady supply of air, water, and food. One solution to these challenges is the use of underground habitats, which could provide natural shielding from radiation and stable temperatures. Solar panels could provide power, and the ground could be used for agriculture.\n",
      "\n",
      "Working on the Moon would also present unique challenges. The low gravity on the Moon (only one-sixth that of Earth) would require a different approach to moving and handling objects. Workers would need specialized tools and equipment to compensate for the lack of gravity. However, the low gravity could also be an advantage, as it would make it easier to move heavy equipment and materials.\n",
      "\n",
      "In terms of potential work on the Moon, there are several possibilities. The Moon has abundant resources, such as helium-3, which could potentially be used as fuel for fusion reactors on Earth. Mining operations could also extract valuable minerals, such as iron, titanium, and aluminum. The Moon could also serve as a base for scientific research, with the lack of atmosphere creating a pristine environment for experiments.\n",
      "\n",
      "Overall, the concept of living and working on the Moon presents both exciting opportunities and significant challenges. However, with continued research and development, it may one day become a reality, expanding human presence beyond our planet and unlocking the mysteries of our universe.\n",
      "\n",
      "module3_output\n",
      "Lunar Science and Exploration Techniques are a fascinating area of study that has captured the attention of scientists and space enthusiasts worldwide. The moon has been a subject of scientific exploration and research for many years, and we have made significant advancements in our understanding of the moon's composition, structure, and history.\n",
      "\n",
      "Lunar Science encompasses a broad range of disciplines, including geology, chemistry, astronomy, and physics, among others. One of the primary objectives of Lunar Science is to gain insight into the moon's formation and evolution. Scientists also seek to better understand the moon's geology and terrain to identify suitable sites for future exploration and human habitation.\n",
      "\n",
      "Exploration Techniques for the lunar surface have evolved over time, starting from observing the moon through telescopes to sending spacecraft and rovers to the lunar surface. NASA's Apollo program in the 1960s and 70s was the first major effort to explore the moon and marked a significant milestone in human space exploration. Astronauts collected samples of lunar rocks and soil, installed scientific instruments, and conducted experiments to gather data on various aspects of the moon's environment.\n",
      "\n",
      "In recent years, there has been a renewed interest in lunar exploration, driven by the vision of establishing a sustained human presence on the moon. NASA's Artemis program aims to return astronauts to the moon by 2024 and establish a sustainable lunar presence by 2028. The program also aims to conduct scientific research and test new technologies that could enable future explores to venture deeper into space, including to Mars.\n",
      "\n",
      "Several new techniques and technologies are being developed to enable these ambitious missions. For example, new materials and designs are being developed for space suits and habitats that can withstand harsh lunar conditions. Autonomous and semi-autonomous rovers are being designed to explore the lunar surface and collect data on the moon's geology and environment. Scientists are also exploring the use of lunar resources, such as water ice, for in-situ resource utilization and to support future missions.\n",
      "\n",
      "In conclusion, Lunar Science and Exploration Techniques are exciting fields that hold immense potential for advancing our understanding of the moon and unlocking new frontiers in space exploration. With continued investment in research and exploration, we can expect to make significant progress in this area in the coming years.\n"
     ]
    }
   ],
   "source": [
    "print(\"module1_output\")\n",
    "print(module1_output)\n",
    "print(\" \")\n",
    "print(\"module2_output\")\n",
    "print(module2_output)\n",
    "print(\"\")\n",
    "print(\"module3_output\")\n",
    "print(module3_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point1: 1. Lunar Geology and Mineralogy\n",
      "Point2: 2. Living and Working on the Moon\n",
      "Point3: 3. Lunar Science and Exploration Techniques\n"
     ]
    }
   ],
   "source": [
    "print(\"Point1:\", point1)\n",
    "print(\"Point2:\", point2)\n",
    "print(\"Point3:\", point3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRIAL 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "module --> subtopics --> contents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using parallel computing  (much faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.llms import OpenAI\n",
    "# from langchain.chains import LLMChain\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# import concurrent.futures\n",
    "\n",
    "\n",
    "# def course_maker(topic_name):\n",
    "#     llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "#     ################################## GET MODULES ##############################################################\n",
    "#     ################# chain1: Topic name(only 5 for now)\n",
    "\n",
    "#     module_prompt = PromptTemplate(\n",
    "#         input_variables=[\"topic\"],\n",
    "#         template=\"Give me the heading of 2 module names for the course in {topic}. Please don't write any description; I only want the headings.\",\n",
    "#     )\n",
    "\n",
    "#     module_chain = LLMChain(llm=llm, prompt=module_prompt)\n",
    "\n",
    "#     module_output = module_chain.predict(topic=topic_name)\n",
    "\n",
    "#     # split the module_output string into separate points\n",
    "#     module_points = module_output.split('\\n')\n",
    "\n",
    "#     print(\"Modules created:\", module_points)\n",
    "\n",
    "#     #####################################  GET SUBTOPICS  #############################################\n",
    "\n",
    "#     # define the prompt template\n",
    "#     subtopic_prompt = PromptTemplate(\n",
    "#         input_variables=[\"module\"],\n",
    "#         template=\"Give me the heading of 2 subtopics to teach in {module}.\",\n",
    "#     )\n",
    "\n",
    "#     # create the LLMChain instance\n",
    "#     subtopic_chain = LLMChain(llm=llm, prompt=subtopic_prompt)\n",
    "\n",
    "#     ################# all modules\n",
    "\n",
    "#     # iterate over the points and generate the module output strings\n",
    "#     subtopic_outputs_list = []\n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         futures = [executor.submit(subtopic_chain.predict, module=module) for module in module_points]\n",
    "#         for future in concurrent.futures.as_completed(futures):\n",
    "#             subtopic_output = future.result()\n",
    "#             subtopic_outputs_list.append(subtopic_output)\n",
    "\n",
    "#     print(\"Subtopics created:\", subtopic_outputs_list)\n",
    "\n",
    "#     ######################################## course content in each subtopic #############################\n",
    "\n",
    "#     # define the prompt template\n",
    "#     content_prompt = PromptTemplate(\n",
    "#         input_variables=[\"subtopic\"],\n",
    "#         template=\"Give me a detailed description of this {subtopic}. Write in bulleted form and make sure to use examples and codes as well as their stp by step output so that any beginner-level student can understand.\",\n",
    "#     )\n",
    "\n",
    "#     # create the LLMChain instance\n",
    "#     content_chain = LLMChain(llm=llm, prompt=content_prompt)\n",
    "#     contents_output_list = []\n",
    "\n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         for subtopics in subtopic_outputs_list:\n",
    "#             subtopics_points = subtopics.split('\\n')\n",
    "#             futures = [executor.submit(content_chain.predict, subtopic=subtopic) for subtopic in subtopics_points]\n",
    "#             for future in concurrent.futures.as_completed(futures):\n",
    "#                 content_output = future.result()\n",
    "#                 contents_output_list.append(content_output)\n",
    "\n",
    "#     # return the results\n",
    "#     return module_output, subtopic_outputs_list, contents_output_list, *module_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def course_maker(topic_name,application,num_days, level_dropdown):\n",
    "    llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "    ################################## GET MODULES ##############################################################\n",
    "    ################# chain1: Topic name(only 5 for now)\n",
    "\n",
    "    module_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"application\", \"module_number\", \"input_1\", \"input_2\"],\n",
    "    template=\"Create a personalized course module for {topic} and {application}.\\n\\nModule {module_number}: {input_1}\\n\\nModule {module_number + 1}: {input_2}\\n\\nPlease provide a heading or title for each module.\",\n",
    ")\n",
    "\n",
    "\n",
    "    module_chain = LLMChain(llm=llm, prompt=module_prompt)\n",
    "\n",
    "    module_output = module_chain.predict(topic=topic_name, application=application)\n",
    "\n",
    "    # split the module_output string into separate points\n",
    "    module_points = module_output.split('\\n')\n",
    "\n",
    "    print(\"Modules created:\", module_points)\n",
    "\n",
    "    #####################################  GET SUBTOPICS  #############################################\n",
    "\n",
    "    # define the prompt template\n",
    "    subtopic_prompt = PromptTemplate(\n",
    "        input_variables=[\"module\", \"level_dropdown\"],\n",
    "        template=\"Give me the heading of 2 subtopics to teach in {module}. The course should be of {level_dropdown} level so give me the subtopics keeping in mind the same\",\n",
    "    )\n",
    "\n",
    "    # create the LLMChain instance\n",
    "    subtopic_chain = LLMChain(llm=llm, prompt=subtopic_prompt)\n",
    "\n",
    "    ################# all modules\n",
    "\n",
    "    # iterate over the points and generate the module output strings\n",
    "    subtopic_outputs_list = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(subtopic_chain.predict, module=module, level_dropdown = level_dropdown) for module in module_points]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            subtopic_output = future.result()\n",
    "            subtopic_outputs_list.append(subtopic_output)\n",
    "\n",
    "    print(\"Subtopics created:\", subtopic_outputs_list)\n",
    "\n",
    "    ######################################## course content in each subtopic #############################\n",
    "\n",
    "    # define the prompt template\n",
    "    content_prompt = PromptTemplate(\n",
    "        input_variables=[\"subtopic\", \"num_days\", \"level_dropdown\"],\n",
    "        template=\"Give me a detailed description of this {subtopic}. The contents should be {num_days} long. The level of contents should be {level_dropdown}. Write in bulleted form and make sure to use examples and codes as well as their step by step output.\",\n",
    "    )\n",
    "\n",
    "        # define the prompt template\n",
    "    ppt_prompt = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"I am planning to make course on the {content}. So I want to you break this down into slides. You can use any number of slides but i want the contents extracted from the {content} and don't write how it can be done, just do it youself and give me the output\")\n",
    "\n",
    "\n",
    "\n",
    "    # define the script prompt\n",
    "    script_prompt = PromptTemplate(\n",
    "        input_variables=[\"ppt\"],\n",
    "        template=\"Give me a video script for this {ppt} with basic explanation and of everything the explanation should be beginner friendly. don't include scene, narrator just write the script\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # create the LLMChain instance\n",
    "    content_chain = LLMChain(llm=llm, prompt=content_prompt)\n",
    "    ppt_chain = LLMChain(llm=llm, prompt= ppt_prompt)\n",
    "    script_chain = LLMChain(llm=llm, prompt=script_prompt)\n",
    "\n",
    "    contents_output_list = []\n",
    "    ppt_output_list = []\n",
    "    scripts_output_list = []\n",
    "\n",
    "    with    () as executor:\n",
    "        for subtopics in subtopic_outputs_list:\n",
    "            subtopics_points = subtopics.split('\\n')\n",
    "            futures = [executor.submit(content_chain.predict, subtopic=subtopic, num_days=num_days, level_dropdown=level_dropdown) for subtopic in subtopics_points]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                content_output = future.result()\n",
    "                ppt_output = ppt_chain.predict(content= content_output)\n",
    "                script_output = script_chain.predict(ppt=ppt_output)\n",
    "\n",
    "                #append script and content in the lists\n",
    "                scripts_output_list.append(script_output)\n",
    "                ppt_output_list.append(ppt_output)\n",
    "                contents_output_list.append(content_output)\n",
    "\n",
    "    # return the results\n",
    "    return module_output, subtopic_outputs_list, contents_output_list,ppt_output_list,scripts_output_list, *module_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules created: ['1. Object Detection and Recognition', '2. Medical Image Analysis']\n",
      "Subtopics created: ['1. Introduction to Object Detection and Recognition\\n2. Image Data Collection and Preprocessing Techniques for Object Detection and Recognition', '1. Basic Image Processing Techniques in Medical Image Analysis \\n2. Introduction to Segmentation and Feature Extraction in Medical Images']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n"
     ]
    }
   ],
   "source": [
    "module_output, subtopic_outputs_list,contents_output_list,ppt_output_list,scripts_output_list, *module_points = course_maker(topic_name='computer vision',application='healthcare',num_days=5, level_dropdown='beginner')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the contents in txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Specify the file path and name\n",
    "# file_path = \"contents_output_list.txt\"\n",
    "\n",
    "# # Open the file in write mode\n",
    "# with open(file_path, 'w') as file:\n",
    "#     # Iterate over the list and write each element to a new line in the file\n",
    "#     for item in contents_output_list:\n",
    "#         file.write(str(item) + '\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store output in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def course_maker(module_points,subtopic_outputs_list,contents_output_list,scripts_output_list ):\n",
    "\n",
    "    # Create a dictionary to store the data\n",
    "    course_data = {}\n",
    "\n",
    "    for i, module in enumerate(module_points):\n",
    "        # Store module output\n",
    "        module_output = module\n",
    "\n",
    "        # Get corresponding subtopics\n",
    "        subtopics = subtopic_outputs_list[i].split('\\n')\n",
    "\n",
    "        module_data = []\n",
    "        for j, subtopic in enumerate(subtopics):\n",
    "            # Store subtopic output\n",
    "            subtopic_output = subtopic\n",
    "\n",
    "            # Get corresponding content \n",
    "            content_output = contents_output_list[i*len(subtopics) + j]\n",
    "            script_output = scripts_output_list[i*len(subtopics) + j]\n",
    "\n",
    "\n",
    "            # Create a dictionary for subtopic and content and scripts\n",
    "            subtopic_data = {\n",
    "                \"subtopic_output\": subtopic_output,\n",
    "                \"content_output\": content_output,\n",
    "                \"script_output\": script_output,\n",
    "            }\n",
    "\n",
    "            # Append subtopic data to module data\n",
    "            module_data.append(subtopic_data)\n",
    "\n",
    "        # Add module data to course_data\n",
    "        course_data[module_output] = module_data\n",
    "\n",
    "    # Save the data as JSON\n",
    "    json_data = json.dumps(course_data, indent=4)\n",
    "\n",
    "    return json_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data =  course_maker(module_points,subtopic_outputs_list,contents_output_list,scripts_output_list)\n",
    "\n",
    "\n",
    "#save json data\n",
    "with open(\"course_full.json\", \"w\") as file:\n",
    "    file.write(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual steps instead of one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9,request_timeout=300)\n",
    "\n",
    "    ################################## GET MODULES ##############################################################\n",
    "    ################# chain1: Topic name(only 5 for now)\n",
    "\n",
    "module_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"application\", \"module_number\", \"level_dropdown\"],\n",
    "    template=\"You want to create a {module_number} module {level_dropdown} level personalized course on {topic} which i will use for {application}. Let's start by designing the module structure and subtopics to include in each module. Write in this format: Module 1 heading then subtopic 1 heading ,subtopic 2 heading,subtopic 3 heading......... give numbering to module and subtopics.Don't write any NOTE in the end just write module name and subtopic name. Give the output in JSON format with key value pairs. The format will be Module name then its heading as value then subkey would be all the subtopics in that module and their heading as values.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "module_chain = LLMChain(llm=llm, prompt=module_prompt)\n",
    "\n",
    "module_output =  module_chain.predict(topic='Large language model', application = 'academic exam', module_number = 2, level_dropdown= 'beginner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Module 1\": {\n",
      "        \"Heading\": \"Introduction to Large Language Model\",\n",
      "        \"Subtopics\": {\n",
      "            \"1\": \"What is a Large Language Model?\",\n",
      "            \"2\": \"Types of Large Language Models\",\n",
      "            \"3\": \"Applications of Large Language Models\"\n",
      "        }\n",
      "    },\n",
      "    \"Module 2\": {\n",
      "        \"Heading\": \"Working with Large Language Model\",\n",
      "        \"Subtopics\": {\n",
      "            \"1\": \"Preprocessing Text Data\",\n",
      "            \"2\": \"Fine-tuning a Large Language Model\",\n",
      "            \"3\": \"Evaluating a Large Language Model\",\n",
      "            \"4\": \"Interpreting the Results\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(module_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module 1: Introduction to Large Language Model\n",
      "Subtopic 3: Applications of Large Language Models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parse the JSON data\n",
    "data = json.loads(module_output)\n",
    "\n",
    "# Access Module 1 and Subtopic 3\n",
    "module_1 = data[\"Module 1\"]\n",
    "subtopic_3 = module_1[\"Subtopics\"][\"3\"]\n",
    "\n",
    "# Print the results\n",
    "print(\"Module 1:\", module_1[\"Heading\"])\n",
    "print(\"Subtopic 3:\", subtopic_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save json data\n",
    "with open(\"module_output.json\", \"w\") as file:\n",
    "    file.write(module_output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create contents for each module and subtopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Heading': 'Introduction to Large Language Model', 'Subtopics': {'1': 'What is a Large Language Model?', '2': 'Types of Large Language Models', '3': 'Applications of Large Language Models'}}\n",
      "{'Heading': 'Working with Large Language Model', 'Subtopics': {'1': 'Preprocessing Text Data', '2': 'Fine-tuning a Large Language Model', '3': 'Evaluating a Large Language Model', '4': 'Interpreting the Results'}}\n"
     ]
    }
   ],
   "source": [
    "# print(module_output)\n",
    "\n",
    "data = json.loads(module_output)\n",
    "\n",
    "for x in data.values():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Module 1\": {\n",
      "        \"Heading\": \"Introduction to Large Language Model\",\n",
      "        \"Subtopics\": {\n",
      "            \"1\": {\n",
      "                \"title\": \"What is a Large Language Model?\",\n",
      "                \"content\": \"A large language model (LLM) is a type of machine learning model that uses deep learning techniques to predict text or speech. It is capable of generating human-like language and can be used for various purposes such as language translation, chatbots, content generation, and more. Some examples of LLMs include GPT-3, BERT, and Transformer.\\n\\nHere are some key features of LLMs:\\n\\n- Pre-trained on large datasets: LLMs require a massive amount of data to learn from, which is why they are pre-trained on large datasets such as Wikipedia, Common Crawl, and other sources of text or speech. This pre-training allows the model to learn the patterns and structures of natural language.\\n\\n- Fine-tuning on specific tasks: After pre-training, LLMs can be fine-tuned on specific tasks such as sentiment analysis, question answering, or chatbot dialogue generation. Fine-tuning involves adjusting the parameters of the model to fit the particular task.\\n\\n- Attention mechanisms: LLMs use attention mechanisms, which allow the model to focus on the most relevant parts of the input sequence when generating output. This improves the quality of the generated text or speech.\\n\\n- Sample code:\\n\\nTo give a better understanding of LLMs, here is some sample code using the popular Python library, Hugging Face Transformers:\\n\\n```python\\n# Import the necessary libraries\\nfrom transformers import pipeline\\n\\n# Load the pre-trained GPT-2 model\\ngenerator = pipeline('text-generation', model='gpt2')\\n\\n# Generate text based on the prompt\\nprompt = \\\"Once upon a time\\\"\\ngenerated_text = generator(prompt, max_length=100)\\n\\n# Print the generated text\\nprint(generated_text[0]['generated_text'])\\n```\\n\\nIn this example, we load the pre-trained GPT-2 model using the Transformers library. Then we generate text based on the prompt \\\"Once upon a time\\\" using the `pipeline` function. Finally, we print the generated text. This code shows how easy it is to use a pre-trained LLM for text generation.\"\n",
      "            },\n",
      "            \"2\": {\n",
      "                \"title\": \"Types of Large Language Models\",\n",
      "                \"content\": \"1. Transformer-based:\\n- Uses the transformer architecture\\n- Examples: GPT, BERT, XLNet\\n- Code example:\\nfrom transformers import pipeline, AutoModelForCausalityLM, AutoTokenizer\\n\\nmodel = AutoModelForCausalityLM.from_pretrained(\\\"EleutherAI/gpt-neo-1.3B\\\")\\ntokenizer = AutoTokenizer.from_pretrained(\\\"EleutherAI/gpt-neo-1.3B\\\")\\n\\ngenerator = pipeline('text-generation', model=model, tokenizer=tokenizer)\\noutput = generator('Hello, ', max_length=50, do_sample=True)\\n\\n2. LSTM-based:\\n- Uses LSTM cells for sequential modeling\\n- Can be unidirectional or bidirectional\\n- Examples: LSTM-LM, ELMo\\n- Code example:\\nfrom keras.models import Sequential\\nfrom keras.layers import LSTM, Dense, Embedding\\n\\nmodel = Sequential()\\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_size))\\nmodel.add(LSTM(units=hidden_size))\\nmodel.add(Dense(units=vocab_size, activation='softmax'))\\n\\n3. Convolutional-based:\\n- Uses convolutional layers for feature extraction\\n- Examples: ByteNet, ConvLM\\n- Code example:\\nfrom keras.models import Sequential\\nfrom keras.layers import Conv1D, GlobalMaxPooling1D, Dense\\n\\nmodel = Sequential()\\nmodel.add(Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu'))\\nmodel.add(GlobalMaxPooling1D())\\nmodel.add(Dense(units=vocab_size, activation='softmax'))\\n\\n4. Markov-based:\\n- Uses Markov models for language modeling\\n- Can be n-gram models or higher-order Markov models\\n- Examples: SRILM, KenLM\\n- Code example:\\nfrom nltk.lm import MLE\\nfrom nltk.lm.preprocessing import padded_everygram_pipeline\\n\\nn = 3\\ntrain_data, padded_sents = padded_everygram_pipeline(n, corpus)\\n\\nmodel = MLE(n)\\nmodel.fit(train_data, padded_sents)\"\n",
      "            },\n",
      "            \"3\": {\n",
      "                \"title\": \"Applications of Large Language Models\",\n",
      "                \"content\": \"1. Text generation\\n- GPT-2 and GPT-3 models can generate text based on a given prompt\\n- Example: GPT-3 can generate a news article based on a headline\\n```\\nimport openai\\n\\nopenai.api_key = \\\"INSERT YOUR API KEY HERE\\\"\\n\\nprompt = \\\"Bitcoin hits new all-time high\\\"\\nmodel = \\\"text-davinci-002\\\"\\n\\nresponse = openai.Completion.create(\\n    model=model,\\n    prompt=prompt,\\n    temperature=0.7,\\n    max_tokens=200\\n)\\n\\ngenerated_text = response.choices[0].text\\nprint(generated_text)\\n```\\n\\n2. Text classification\\n- BERT and RoBERTa models can classify text into different categories\\n- Example: BERT can classify a tweet as being positive or negative\\n```\\n!pip install transformers\\n\\nfrom transformers import pipeline\\n\\nclassifier = pipeline('sentiment-analysis', model='bert-base-uncased')\\n\\nresult = classifier('I love this movie!')\\nprint(result)\\n```\\n\\n3. Question answering\\n- T5 and BERT models can answer questions based on a given context\\n- Example: T5 can answer a question about a Wikipedia article\\n```\\nimport openai\\n\\nopenai.api_key = \\\"INSERT YOUR API KEY HERE\\\"\\n\\nprompt = \\\"Article: The Beatles were an English rock band formed in Liverpool in 1960. Question: When were The Beatles formed?\\\"\\nmodel = \\\"text-davinci-002\\\"\\n\\nresponse = openai.Completion.create(\\n    model=model,\\n    prompt=prompt,\\n    temperature=0.7,\\n    max_tokens=200\\n)\\n\\nanswer = response.choices[0].text\\nprint(answer)\\n```\\n\\n4. Language translation\\n- T5 and Marian models can translate text from one language to another\\n- Example: Marian can translate an English sentence to French\\n```\\nfrom transformers import MarianMTModel, MarianTokenizer\\n\\nmodel_name = 'Helsinki-NLP/opus-mt-en-fr'\\nmodel = MarianMTModel.from_pretrained(model_name)\\ntokenizer = MarianTokenizer.from_pretrained(model_name)\\n\\ntext = \\\"Hello, how are you?\\\"\\ninput_ids = tokenizer(text, return_tensors=\\\"pt\\\").input_ids\\noutput_ids = model.generate(input_ids)[0]\\noutput_text = tokenizer.decode(output_ids, skip_special_tokens=True)\\nprint(output_text)\\n```\\n\\n5. Sentiment analysis\\n- VADER and TextBlob models can analyze the sentiment of a text\\n- Example: VADER can analyze the sentiment of a tweet\\n```\\n!pip install vaderSentiment\\n\\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\\n\\nanalyzer = SentimentIntensityAnalyzer()\\n\\ntext = \\\"I love this movie!\\\"\\nsentiment = analyzer.polarity_scores(text)\\n\\nprint(sentiment)\\n```\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"Module 2\": {\n",
      "        \"Heading\": \"Working with Large Language Model\",\n",
      "        \"Subtopics\": {\n",
      "            \"1\": {\n",
      "                \"title\": \"Preprocessing Text Data\",\n",
      "                \"content\": \"Preprocessing text data is a crucial step in natural language processing (NLP) tasks. It involves cleaning and transforming raw text data into a format that can be easily processed by machine learning algorithms. The following are some common preprocessing steps for text data:\\n\\n1. Import necessary libraries:\\n\\n```python\\nimport re\\nimport string\\nimport nltk\\nfrom nltk.corpus import stopwords\\nfrom nltk.stem import WordNetLemmatizer\\n```\\n\\n2. Load the dataset:\\n\\n```python\\nwith open('text_file.txt', 'r') as file:\\n    text = file.read()\\n```\\n\\n3. Convert text data to lowercase:\\n\\n```python\\ntext = text.lower()\\n```\\n\\n4. Remove punctuation:\\n\\n```python\\ntext = text.translate(str.maketrans('', '', string.punctuation))\\n```\\n\\n5. Remove numerical values:\\n\\n```python\\ntext = re.sub(r'\\\\d+', '', text)\\n```\\n\\n6. Tokenize the text:\\n\\n```python\\ntokens = nltk.word_tokenize(text)\\n```\\n\\n7. Remove stop words:\\n\\n```python\\nstop_words = set(stopwords.words('english'))\\nfiltered_tokens = [word for word in tokens if not word in stop_words]\\n```\\n\\n8. Lemmatize words:\\n\\n```python\\nlemmatizer = WordNetLemmatizer()\\nlemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\\n```\\n\\n9. Join tokens back into a string:\\n\\n```python\\nclean_text = ' '.join(lemmatized_tokens)\\n```\\n\\n10. Save the cleaned text data:\\n\\n```python\\nwith open('clean_text_file.txt', 'w') as file:\\n    file.write(clean_text)\\n``` \\n\\nOverall, preprocessing text data requires various techniques and methods to clean and transform the data. These steps can improve the accuracy of NLP models and better understand natural language.\"\n",
      "            },\n",
      "            \"2\": {\n",
      "                \"title\": \"Fine-tuning a Large Language Model\",\n",
      "                \"content\": \"Fine-tuning a large language model involves taking a pre-trained language model and training it on a smaller, domain-specific dataset to improve its performance on that particular task. Here is a step-by-step guide on how to fine-tune a large language model:\\n\\n1. Import the pre-trained model: First, import the pre-trained language model and set up the tokenizer. Here we use the GPT-2 model from the transformers library:\\n\\n```\\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\\n\\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\\n```\\n\\n2. Load the training dataset: Next, load the training dataset and tokenize it using the pre-trained tokenizer:\\n\\n```\\ntrain_dataset = load_dataset('text', split='train')\\ntrain_encodings = tokenizer(train_dataset['text'], truncation=True, padding=True)\\n```\\n\\n3. Fine-tune the model: Now it's time to fine-tune the model on the smaller dataset. Here we use the Trainer class from the transformers library to handle the training process:\\n\\n```\\nfrom transformers import Trainer, TrainingArguments\\n\\ntraining_args = TrainingArguments(\\n    output_dir='./results',          # output directory\\n    num_train_epochs=3,              # total number of training epochs\\n    per_device_train_batch_size=16,  # batch size per device during training\\n    per_device_eval_batch_size=64,   # batch size for evaluation\\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\\n    weight_decay=0.01,               # strength of weight decay\\n    logging_dir='./logs',            # directory for storing logs\\n    logging_steps=10,\\n)\\n\\ntrainer = Trainer(\\n    model=model,                         # the instantiated Transformers model to be trained\\n    args=training_args,                  # training arguments, defined above\\n    train_dataset=train_encodings['input_ids'],         # training dataset\\n    data_collator=lambda data: {'input_ids': torch.stack(data)}, # data collator\\n)\\n```\\n\\n4. Evaluate the model: After the model has been fine-tuned, it's a good idea to evaluate its performance on a validation dataset:\\n\\n```\\neval_dataset = load_dataset('text', split='validation')\\neval_encodings = tokenizer(eval_dataset['text'], truncation=True, padding=True)\\n\\ntrainer.evaluate(eval_encodings['input_ids'])\\n```\\n\\n5. Generate text: Finally, we can generate text using the fine-tuned model:\\n\\n```\\ngenerated_text = model.generate(\\n    input_ids=tokenizer.encode('I like to', return_tensors='pt'),\\n    max_length=50,\\n    num_beams=5,\\n    no_repeat_ngram_size=2,\\n    early_stopping=True\\n)\\n\\nprint(tokenizer.decode(generated_text[0], skip_special_tokens=True))\\n```\\n\\nBy following these steps, you can fine-tune a large language model for a specific task and improve its performance.\"\n",
      "            },\n",
      "            \"3\": {\n",
      "                \"title\": \"Evaluating a Large Language Model\",\n",
      "                \"content\": \"Evaluating a Large Language Model\\n\\n1. Introduction\\n   * Evaluation is important to ensure the validity and effectiveness of a language model.\\n   * A large language model involves the use of complex algorithms and massive amounts of data, making evaluation critical to its success.\\n\\n2. Metrics for Evaluation\\n   * Perplexity - measures how well the model predicts the next word in a sequence.\\n   * F1 Score - measures the accuracy of the model in identifying entities in text.\\n   * BLEU Score - measures the similarity of machine-generated text to human-written text.\\n\\n3. Preparing Data for Evaluation\\n   * Data cleaning - removes unwanted data and transforms the text into a standardized format.\\n   * Data partition - separates data into training, validation, and test sets.\\n\\n4. Evaluating the Model\\n   * Load the model - loads the language model using a pre-trained model or training a new one.\\n   * Test the model - uses the test data set to evaluate the model's performance based on the metrics chosen.\\n   * Example Code:\\n   \\n   ```\\n   from transformers import pipeline, set_seed\\n   \\n   generator = pipeline('text-generation', model='gpt2')\\n   set_seed(42)\\n   \\n   prompt = \\\"The cat sat on the\\\"\\n   output = generator(prompt, max_length=20, do_sample=True, temperature=0.7)\\n   print(output[0]['generated_text'])\\n   \\n   # Output: \\n   # The cat sat on the couch, purring contentedly. \\n     \\n   ```\\n   \\n5. Interpreting the Results\\n   * Analyze the metrics obtained from the evaluation process to determine the effectiveness of the language model.\\n   * Use the results to identify areas for improvement and refine the model accordingly.\\n\\n6. Conclusion\\n   * Evaluation is a critical component in building large language models that can effectively learn and predict text data.\\n   * The use of appropriate metrics, data preparation, and analysis are essential to ensure the validity and accuracy of the model.\"\n",
      "            },\n",
      "            \"4\": {\n",
      "                \"title\": \"Interpreting the Results\",\n",
      "                \"content\": \"Interpreting the results of data analysis is a crucial step in the data science process. Following are the steps to interpret the results of data analysis.\\n\\n1. Understand the data set - Before interpreting the results, it is crucial to understand the data set and its characteristics. It helps to provide useful insights and context to the results.\\n\\n2. Analyze the descriptive statistics \\u2013 Descriptive statistics help to understand the central tendency, the spread of data, and the overall distribution of the data set. For example, mean, median, mode, standard deviation, variance, etc.\\n\\n3. Use graphs and visualizations - Visualizations, such as histograms, scatter plots can help to understand the trends and patterns that are not visible in numerical data. For example, the below code generates a histogram of the data.\\n\\n```Python\\nimport matplotlib.pyplot as plt\\nplt.hist(data)\\nplt.show()\\n```\\n\\n4. Check the statistical significance - Statistical significance helps to determine if the results are not due to chance. For example, the below code calculates the p-value of a t-test.\\n\\n```Python\\nfrom scipy.stats import ttest_ind\\nt, p = ttest_ind(group1, group2)\\nprint('p-value:',p)\\n```\\n\\n5. Interpret the results - Based on the analysis, interpret the results and make recommendations. For example, If p < 0.05, reject the null hypothesis, which means there is a statistically significant relationship between the two groups.\\n\\n6. Test the assumptions - Always test the assumptions made during data analysis. For example, normality, homogeneity of variance, independence, etc. If assumptions are not met, the results may not be valid.\\n\\n7. Document the findings \\u2013 Finally, document the findings to communicate the results with stakeholders. Use visualizations, charts, and clear language that can be easily understood by non-technical persons. \\n\\nIn conclusion, interpreting the results is a critical step in the data science process. By following these steps, data scientists can provide useful insights and actionable recommendations that can drive business decisions.\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "# Define the content_prompt template\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"subtopic\"],\n",
    "    template=\"Give me a detailed description of {subtopic}. Write in bulleted form and make sure to use code examples so that any beginner-level student can understand.\"\n",
    ")\n",
    "\n",
    "# Create the LLMChain instance\n",
    "content_chain = LLMChain(llm=llm, prompt=content_prompt)\n",
    "\n",
    "data = json.loads(module_output)\n",
    "\n",
    "# Function to generate content for a subtopic\n",
    "def generate_content(subtopic):\n",
    "    content = content_chain.predict(subtopic=subtopic)\n",
    "    return {\n",
    "        \"title\": subtopic,\n",
    "        \"content\": content\n",
    "    }\n",
    "\n",
    "# Generate contents for each subtopic in every module using ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    for module in data.values():\n",
    "        subtopics = module[\"Subtopics\"]\n",
    "        # Create a list of futures for content generation\n",
    "        futures = [executor.submit(generate_content, subtopic) for subtopic in subtopics.values()]\n",
    "        # Retrieve the results from the futures and update the subtopics\n",
    "        for future, (subtopic_number, _) in zip(futures, subtopics.items()):\n",
    "            content = future.result()\n",
    "            subtopics[subtopic_number] = content\n",
    "\n",
    "# Save the updated module_output to a JSON file\n",
    "with open(\"module_output.json\", \"w\") as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Print the updated module_output\n",
    "print(json.dumps(data, indent=4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "content splitted into slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Module 1\": {\n",
      "        \"Heading\": \"Introduction to Large Language Model\",\n",
      "        \"Subtopics\": {\n",
      "            \"1\": {\n",
      "                \"title\": \"What is a Large Language Model?\",\n",
      "                \"full content\": \"A large language model is a type of artificial intelligence (AI) tool that uses machine learning techniques to process and understand human language. These models are trained on massive amounts of text data to learn patterns and relationships in language. They can then use this knowledge to generate new text, answer questions, or perform other language-related tasks.\\n\\nSome examples of large language models include:\\n\\n- OpenAI's GPT-3: a neural network-based model that has been trained on a huge amount of text data and can generate human-like text in response to prompts.\\n- Google's BERT: a transformer-based model that can perform a variety of language-related tasks, including question answering and sentiment analysis.\\n- Hugging Face's transformers: a library of pre-trained language models that can be fine-tuned on specific tasks.\\n\\nHere are some key features and benefits of large language models:\\n\\n- They can generate human-like text: because they have learned patterns and relationships in language, large language models can generate text that is very convincing and can sometimes be difficult to distinguish from text written by a human.\\n- They can perform a wide range of language-related tasks: depending on how they have been trained and fine-tuned, large language models can perform tasks like language translation, summarization, sentiment analysis, and more.\\n- They can be used in many applications: large language models are used in a variety of applications, from chatbots to virtual assistants to content generation tools.\\n\\nHere is some sample code that demonstrates how to use the transformers library to generate text with a pre-trained language model:\\n\\n```\\nfrom transformers import pipeline\\n\\n# load a pre-trained language model\\ngenerator = pipeline('text-generation', model='gpt2')\\n\\n# generate some text based on a prompt\\ntext = generator('The meaning of life is', max_length=50, num_return_sequences=1)\\n\\n# print the generated text\\nprint(text[0]['generated_text'])\\n```\\n\\nIn this code, we load a pre-trained language model (in this case, OpenAI's GPT-2) using the transformers library. We then use this model to generate some text based on a prompt (\\\"The meaning of life is\\\"). The `max_length` parameter controls the length of the generated text, while `num_return_sequences` controls how many different sequences of text to generate. Finally, we print out the generated text.\",\n",
      "                \"split content\": \"Slide 1: Introduction\\n- A large language model is an AI tool that uses machine learning techniques to process and understand human language.\\n- These models are trained on massive amounts of text data to learn patterns and relationships in language.\\n- They can be used for a variety of language-related tasks, including generating new text and answering questions.\\n\\nSlide 2: Examples of large language models\\n- OpenAI's GPT-3: a neural network-based model that generates human-like text in response to prompts.\\n- Google's BERT: a transformer-based model that performs a variety of language-related tasks, including question answering and sentiment analysis.\\n- Hugging Face's transformers: a library of pre-trained language models that can be fine-tuned on specific tasks.\\n\\nSlide 3: Key features and benefits of large language models\\n- They can generate human-like text.\\n- They can perform a wide range of language-related tasks, from translation to summarization to sentiment analysis.\\n- They can be used in a variety of applications, such as chatbots and virtual assistants.\\n\\nSlide 4: Sample code for generating text with a pre-trained language model\\n- The transformers library can be used to load and generate text with a pre-trained language model.\\n- The `max_length` and `num_return_sequences` parameters can be adjusted to control the length and number of generated text sequences.\\n- It is important to choose the appropriate pre-trained language model for the task at hand.\\n\\nSlide 5: Applications of large language models\\n- Chatbots and virtual assistants can use large language models to generate human-like responses to user input.\\n- Content generation tools can use large language models to automatically generate written content.\\n- Language translation and summarization services can use large language models to improve accuracy and efficiency.\\n\\nSlide 6: Challenges of large language models\\n- Large language models require massive amounts of training data and computational resources to develop and maintain.\\n- They may perpetuate or amplify biases present in the training data.\\n- They may raise ethical concerns about the use of AI-generated text.\"\n",
      "            },\n",
      "            \"2\": {\n",
      "                \"title\": \"Types of Large Language Models\",\n",
      "                \"full content\": \"1. Transformer-based models\\n- Based on the Transformer architecture introduced by Vaswani et al. in 2017\\n- Popular models include GPT-2, GPT-3, and BERT\\n- Example code:\\n\\n```\\nfrom transformers import GPT2Model, GPT2Tokenizer\\n\\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\\nmodel = GPT2Model.from_pretrained('gpt2')\\n```\\n\\n2. LSTM-based models\\n- Based on the Long Short-Term Memory (LSTM) architecture introduced by Hochreiter and Schmidhuber in 1997\\n- Popular models include LSTM-LM and ELMO\\n- Example code:\\n\\n```\\nfrom keras.models import Sequential\\nfrom keras.layers import LSTM, Dense\\n\\nmodel = Sequential()\\nmodel.add(LSTM(256, input_shape=(seq_len, vocab_size)))\\nmodel.add(Dense(vocab_size, activation='softmax'))\\n```\\n\\n3. Convolutional-based models\\n- Based on the Convolutional Neural Network (CNN) architecture often used for image classification\\n- Popular models include ByteNet and ConvS2S\\n- Example code:\\n\\n```\\nimport torch.nn as nn\\n\\nclass CNN_LM(nn.Module):\\n    def __init__(self, seq_len, vocab_size):\\n        super().__init__()\\n        self.conv1 = nn.Conv1d(seq_len, 128, kernel_size=3, padding=1)\\n        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\\n        self.linear = nn.Linear(256 * seq_len, vocab_size)\\n\\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = nn.functional.relu(x)\\n        x = self.conv2(x)\\n        x = nn.functional.relu(x)\\n        x = x.view(x.shape[0], -1)\\n        x = self.linear(x)\\n        return x\\n```\",\n",
      "                \"split content\": \"Slide 1:\\n- Introduction to transformer-based models\\n- Brief overview of the Transformer architecture introduced by Vaswani et al. in 2017\\n- Mention popular models including GPT-2, GPT-3, and BERT\\n- Show example code:\\n\\n```\\nfrom transformers import GPT2Model, GPT2Tokenizer\\n\\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\\nmodel = GPT2Model.from_pretrained('gpt2')\\n```\\n\\nSlide 2:\\n- Introduction to LSTM-based models\\n- Brief overview of the Long Short-Term Memory (LSTM) architecture introduced by Hochreiter and Schmidhuber in 1997\\n- Mention popular models including LSTM-LM and ELMO\\n- Show example code:\\n\\n```\\nfrom keras.models import Sequential\\nfrom keras.layers import LSTM, Dense\\n\\nmodel = Sequential()\\nmodel.add(LSTM(256, input_shape=(seq_len, vocab_size)))\\nmodel.add(Dense(vocab_size, activation='softmax'))\\n```\\n\\nSlide 3:\\n- Introduction to convolutional-based models\\n- Brief overview of the Convolutional Neural Network (CNN) architecture often used for image classification\\n- Mention popular models including ByteNet and ConvS2S\\n- Show example code:\\n\\n```\\nimport torch.nn as nn\\n\\nclass CNN_LM(nn.Module):\\n    def __init__(self, seq_len, vocab_size):\\n        super().__init__()\\n        self.conv1 = nn.Conv1d(seq_len, 128, kernel_size=3, padding=1)\\n        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\\n        self.linear = nn.Linear(256 * seq_len, vocab_size)\\n\\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = nn.functional.relu(x)\\n        x = self.conv2(x)\\n        x = nn.functional.relu(x)\\n        x = x.view(x.shape[0], -1)\\n        x = self.linear(x)\\n        return x\\n```\"\n",
      "            },\n",
      "            \"3\": {\n",
      "                \"title\": \"Applications of Large Language Models\",\n",
      "                \"full content\": \"1. Language Generation\\n- Generate text for chatbots, virtual assistants, and customer service interactions\\n- Example: GPT-3 can generate news articles, poetry, and even computer code given a prompt\\n\\n2. Auto-translation\\n- Translate text between languages\\n- Example: Google Translate uses a transformer model to translate text between languages\\n\\n3. Sentiment Analysis\\n- Analyze the emotion of text\\n- Example: VaderSentiment is a Python package that uses a pre-trained model to analyze the sentiment of text\\n\\n4. Summarization\\n- Summarize large amounts of text\\n- Example: BART can summarize news articles and scientific papers\\n\\n5. Question Answering\\n- Answer questions based on text\\n- Example: OpenAI's GPT-3 can answer trivia questions and give advice on a variety of topics\\n\\n6. Text Classification\\n- Categorize text into different topics or classes\\n- Example: TensorFlow's text classification tutorial shows how to classify movie reviews into positive or negative categories\\n\\n7. Named Entity Recognition\\n- Identify and extract entities from text, such as people, places, and dates\\n- Example: spaCy is a Python library that uses pre-trained models to identify named entities in text \\n\\n8. Speech Recognition\\n- Convert spoken words into text \\n- Example: Google's Speech-to-Text API can transcribe spoken words into text for applications like automated captions and voice search\\n\\n9. Language Modelling\\n- Predict the next word given a sequence of text\\n- Example: LSTM neural networks can be used to generate text, such as song lyrics or movie scripts\\n\\n10. Text Clustering and Topic Modeling\\n- Group similar documents together and identify key topics in large corpora of text\\n- Example: Scikit-learn's clustering module provides algorithms for clustering text data based on similarities in their features, such as word frequencies.\",\n",
      "                \"split content\": \"Slide 1:\\n- Introduction to Natural Language Processing (NLP) Techniques\\n- Brief explanation of Language Generation\\n\\nSlide 2:\\n- Language Generation\\n- Use cases: chatbots, virtual assistants, and customer service interactions\\n- Example: GPT-3 can generate news articles, poetry, and even computer code given a prompt\\n\\nSlide 3:\\n- Auto-translation\\n- Translate text between languages\\n- Example: Google Translate uses a transformer model to translate text between languages\\n\\nSlide 4:\\n- Sentiment Analysis\\n- Analyze the emotion of text\\n- Example: VaderSentiment is a Python package that uses a pre-trained model to analyze the sentiment of text\\n\\nSlide 5:\\n- Summarization\\n- Summarize large amounts of text\\n- Example: BART can summarize news articles and scientific papers\\n\\nSlide 6:\\n- Question Answering\\n- Answer questions based on text\\n- Example: OpenAI's GPT-3 can answer trivia questions and give advice on a variety of topics\\n\\nSlide 7:\\n- Text Classification\\n- Categorize text into different topics or classes\\n- Example: TensorFlow's text classification tutorial shows how to classify movie reviews into positive or negative categories\\n\\nSlide 8:\\n- Named Entity Recognition\\n- Identify and extract entities from text, such as people, places, and dates\\n- Example: spaCy is a Python library that uses pre-trained models to identify named entities in text \\n\\nSlide 9:\\n- Speech Recognition\\n- Convert spoken words into text \\n- Example: Google's Speech-to-Text API can transcribe spoken words into text for applications like automated captions and voice search\\n\\nSlide 10:\\n- Language Modelling\\n- Predict the next word given a sequence of text\\n- Example: LSTM neural networks can be used to generate text, such as song lyrics or movie scripts\\n\\nSlide 11:\\n- Text Clustering and Topic Modeling\\n- Group similar documents together and identify key topics in large corpora of text\\n- Example: Scikit-learn's clustering module provides algorithms for clustering text data based on similarities in their features, such as word frequencies.\\n\\nSlide 12: \\n- Conclusion: NLP techniques can help with a variety of tasks like language generation, sentiment analysis, and text classification. These techniques can be applied in various industries like customer service, marketing, and healthcare.\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"Module 2\": {\n",
      "        \"Heading\": \"Working with Large Language Model\",\n",
      "        \"Subtopics\": {\n",
      "            \"1\": {\n",
      "                \"title\": \"Preprocessing Text Data\",\n",
      "                \"full content\": \"Preprocessing Text Data involves cleaning and transforming raw text data into a usable format for further analysis or NLP model training. The following are some key steps involved in preprocessing text data:\\n\\n1. Tokenization - Splitting text into individual words or phrases called \\\"tokens\\\". \\n\\n```python\\nimport nltk\\nnltk.download('punkt')\\nfrom nltk.tokenize import word_tokenize\\n\\ntext = \\\"This is an example sentence.\\\"\\ntokens = word_tokenize(text)\\nprint(tokens)\\n# Output: ['This', 'is', 'an', 'example', 'sentence', '.']\\n```\\n\\n2. Lowercasing - Converting all text to lowercase to ensure consistency.\\n\\n```python\\ntext = \\\"This is an EXAMPLE sentence.\\\"\\ntext = text.lower()\\nprint(text)\\n# Output: 'this is an example sentence.'\\n```\\n\\n3. Stop Word Removal - Removing common words like \\\"the\\\", \\\"and\\\", or \\\"is\\\", that typically do not have a significant impact on the analysis.\\n\\n```python\\nnltk.download('stopwords')\\nfrom nltk.corpus import stopwords\\n\\nstop_words = set(stopwords.words('english'))\\ntokens = [word for word in tokens if word not in stop_words]\\nprint(tokens)\\n# Output: ['This', 'example', 'sentence', '.']\\n```\\n\\n4. Stemming/Lemmatization - Reducing words to their base form to avoid redundancy and improve processing efficiency.\\n\\n```python\\nfrom nltk.stem import PorterStemmer\\nfrom nltk.stem import WordNetLemmatizer\\n\\nps = PorterStemmer()\\nwnl = WordNetLemmatizer()\\n\\nstemmed_tokens = [ps.stem(word) for word in tokens]\\nprint(stemmed_tokens)\\n# Output: ['thi', 'exampl', 'sentenc', '.']\\n\\nlemmatized_tokens = [wnl.lemmatize(word) for word in tokens]\\nprint(lemmatized_tokens)\\n# Output: ['This', 'example', 'sentence', '.']\\n```\\n\\n5. Removing Special Characters and Punctuation - Removing non-alphabetic characters and common punctuation marks.\\n\\n```python\\nimport string\\n\\ntext = \\\"This, is an example sentence!!\\\"\\ntext = text.translate(str.maketrans(\\\"\\\", \\\"\\\", string.punctuation))\\nprint(text)\\n# Output: 'This is an example sentence'\\n```\\n\\n6. Handling Misspelled Words - Correcting common misspellings to improve accuracy.\\n\\n```python\\n!pip install autocorrect\\nfrom autocorrect import Speller\\n\\nspell = Speller()\\n\\ntext = \\\"Ths is an xample sentnc.\\\"\\ncorrected_text = spell(text)\\nprint(corrected_text)\\n# Output: 'This is an example sentence.'\\n```\\n\\nThese are some basic steps involved in preprocessing text data. There may be additional steps depending on the specific task or data being analyzed.\",\n",
      "                \"split content\": \"Slide 1:\\nTitle: Preprocessing Text Data\\n\\n- Introduction: Explain what preprocessing text data means and why it's important.\\n- Explain the basic steps involved in preprocessing text data.\\n- Mention that there may be additional steps depending on the specific task or data being analyzed.\\n\\nSlide 2:\\nTitle: Tokenization\\n\\n- Define tokenization.\\n- Explain how to tokenize text using nltk's word_tokenize function.\\n- Show an example code snippet for tokenization.\\n- Provide an output example.\\n\\nSlide 3:\\nTitle: Lowercasing\\n\\n- Define lowercasing.\\n- Explain why it's important for consistency.\\n- Show an example code snippet for lowercasing.\\n- Provide an output example.\\n\\nSlide 4:\\nTitle: Stop Word Removal\\n\\n- Define stop words.\\n- Explain why removing common words like \\\"the\\\", \\\"and\\\", or \\\"is\\\" is important for analysis.\\n- Show an example code snippet for stop word removal.\\n- Provide an output example.\\n\\nSlide 5:\\nTitle: Stemming/Lemmatization\\n\\n- Define stemming and lemmatization.\\n- Explain why reducing words to their base form is important for avoiding redundancy and improving processing efficiency.\\n- Show an example code snippet for stemming and lemmatization.\\n- Provide an output example.\\n\\nSlide 6:\\nTitle: Removing Special Characters and Punctuation\\n\\n- Explain why removing non-alphabetic characters and common punctuation marks is important.\\n- Show an example code snippet for removing special characters and punctuation.\\n- Provide an output example.\\n\\nSlide 7:\\nTitle: Handling Misspelled Words\\n\\n- Explain how correcting common misspellings can improve accuracy.\\n- Show an example code snippet for handling misspelled words using the autocorrect package.\\n- Provide an output example.\\n\\nSlide 8:\\nTitle: Additional Preprocessing Techniques\\n\\n- Mention that there may be additional preprocessing techniques depending on the specific task or data being analyzed.\\n- Provide examples of additional preprocessing techniques such as removing HTML tags, removing digits, and handling emojis.\\n- Show example code snippets for each additional technique.\\n\\nSlide 9:\\nTitle: Conclusion\\n\\n- Recap the main steps involved in preprocessing text data.\\n- Emphasize the importance of preprocessing for accurate analysis and NLP model training.\\n- Encourage further exploration and experimentation with preprocessing techniques.\"\n",
      "            },\n",
      "            \"2\": {\n",
      "                \"title\": \"Fine-tuning a Large Language Model\",\n",
      "                \"full content\": \"Fine-tuning a large language model is a process of training a pre-trained language model on a specific task, with the aim of improving its performance on that task. Here's a detailed description of the steps involved in fine-tuning a large language model:\\n\\n1. Choose a pre-trained language model: There are many pre-trained language models available, such as GPT-2, BERT, and RoBERTa. You can choose the one that best suits your needs.\\n\\n2. Prepare your data: The data you use to fine-tune the language model should be in the same format as that used to train the pre-trained model. You'll also need to create a training set, a validation set, and a test set.\\n\\n3. Load the pre-trained model: You can load a pre-trained model using the Hugging Face transformers library. Here's an example of how to load a pre-trained GPT-2 model:\\n\\n```\\nfrom transformers import GPT2Tokenizer, GPT2Model\\n\\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\\nmodel = GPT2Model.from_pretrained('gpt2')\\n```\\n\\n4. Prepare the data for fine-tuning: You'll need to tokenize the input data using the tokenizer object you loaded in the previous step. Here's an example of how to tokenize a sentence using the GPT-2 tokenizer:\\n\\n```\\ninput_sentence = \\\"The quick brown fox jumps over the lazy dog.\\\"\\ninput_ids = tokenizer.encode(input_sentence, add_special_tokens=True)\\n```\\n\\n5. Fine-tune the model: Once you've prepared your data, you can fine-tune the pre-trained model on your specific task. Here's an example of how to fine-tune a GPT-2 model on a text generation task:\\n\\n```\\nfrom transformers import TextDataset, DataCollatorForLanguageModeling\\nfrom transformers import Trainer, TrainingArguments\\n\\ntrain_dataset = TextDataset(\\n    tokenizer=tokenizer,\\n    file_path='train.txt',\\n    block_size=128\\n)\\n\\nvalid_dataset = TextDataset(\\n    tokenizer=tokenizer,\\n    file_path='valid.txt',\\n    block_size=128\\n)\\n\\ndata_collator = DataCollatorForLanguageModeling(\\n    tokenizer=tokenizer, mlm=False\\n)\\n\\ntraining_args = TrainingArguments(\\n    output_dir='./results',\\n    overwrite_output_dir=True,\\n    num_train_epochs=1,\\n    per_device_train_batch_size=16,\\n    per_device_eval_batch_size=16,\\n    eval_steps=1000,\\n    save_steps=1000,\\n    warmup_steps=500,\\n    learning_rate=1e-4,\\n    adam_epsilon=1e-8,\\n    weight_decay=0.01,\\n)\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=train_dataset,\\n    eval_dataset=valid_dataset,\\n    data_collator=data_collator,\\n)\\n\\ntrainer.train()\\n```\\n\\n6. Evaluate the model: Once you've fine-tuned the model, you can evaluate its performance on a test set. Here's an example of how to generate text using a fine-tuned GPT-2 model:\\n\\n```\\ninput_ids = tokenizer.encode(\\\"The quick brown fox\\\", return_tensors='pt')\\noutputs = model.generate(input_ids, max_length=50, do_sample=True)\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\nprint(generated_text)\\n```\\n\\nFine-tuning a large language model can be a time-consuming process, but it can yield significant improvements in performance on specific tasks. With the Hugging Face transformers library, it's relatively easy to fine-tune pre-trained models, even for beginners.\",\n",
      "                \"split content\": \"Slide 1:\\nTitle\\nFine-tuning a large language model\\n\\nSlide 2:\\nIntroduction\\n- Definition of fine-tuning a language model\\n- Importance of fine-tuning for improving performance on specific tasks\\n\\nSlide 3:\\nChoosing a pre-trained language model\\n- Examples of pre-trained language models (GPT-2, BERT, RoBERTa)\\n- How to choose the best pre-trained model for your needs\\n\\nSlide 4:\\nPreparing your data\\n- Format of data for fine-tuning should be the same as that used to train pre-trained model\\n- Creating training set, validation set, and test set\\n\\nSlide 5:\\nLoading the pre-trained model\\n- Using the Hugging Face transformers library to load a pre-trained model\\n- Example code for loading a pre-trained GPT-2 model\\n\\nSlide 6:\\nPreparing the data for fine-tuning\\n- Tokenizing input data using the tokenizer object loaded in step 5\\n- Example code for tokenizing a sentence using the GPT-2 tokenizer\\n\\nSlide 7:\\nFine-tuning the model\\n- Process of fine-tuning a pre-trained model on a specific task\\n- Example code for fine-tuning a GPT-2 model on a text generation task\\n\\nSlide 8:\\nEvaluating the model\\n- Evaluating the performance of the fine-tuned model on a test set\\n- Example code for generating text using a fine-tuned GPT-2 model\\n\\nSlide 9:\\nConclusion\\n- Fine-tuning a large language model can be time-consuming but can yield significant improvements in performance on specific tasks\\n- The Hugging Face transformers library makes it relatively easy to fine-tune pre-trained models, even for beginners.\\n\\nSlide 10:\\nReferences\\n- List of sources used for information in the presentation.\"\n",
      "            },\n",
      "            \"3\": {\n",
      "                \"title\": \"Evaluating a Large Language Model\",\n",
      "                \"full content\": \"Evaluating a large language model involves training the model on a large corpus of data and assessing its performance on various language tasks. The most common approach to evaluating language models is to use metrics such as accuracy, perplexity, and F1-score. \\n\\nHere are the steps to evaluate a large language model:\\n\\n1. Prepare your data: The first step is to prepare the dataset that will be used to train and test the model. This involves cleaning the data, removing any irrelevant information, and converting it into a format that the model can understand. \\n\\n2. Train your model: Once the data is prepared, train your model on the dataset. You can use different architectures like RNNs, LSTMs, or transformers to train the model. Here is an example of training a transformer model:\\n\\n```python\\nfrom transformers import GPT2Tokenizer, TFGPT2LMHeadModel\\n\\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\\nmodel = TFGPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)\\n\\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), \\n              loss=model.compute_loss)\\n\\nmodel.fit(train_dataset, epochs=5, validation_data=val_dataset)\\n```\\n\\n3. Test your model: After training the model, evaluate its performance on a test set. This will give you an idea of how well the model has learned to predict the next word in a sequence. Here is an example of testing the model:\\n\\n```python\\ndef generate_text(model, tokenizer, prompt):\\n    input_ids = tokenizer.encode(prompt, return_tensors='tf')\\n    max_length = len(input_ids[0]) + 20\\n    output = model.generate(input_ids, max_length=max_length, do_sample=True)\\n    return tokenizer.decode(output[0])\\n\\nprompt = 'The weather is nice today'\\noutput = generate_text(model, tokenizer, prompt)\\nprint(output)\\n```\\n\\n4. Evaluate the model: Finally, evaluate the performance of your language model using different metrics like accuracy, perplexity, and F1-score. Here is an example of calculating perplexity:\\n\\n```python\\ndef calculate_perplexity(model, tokenizer, test_dataset):\\n    test_loss = 0\\n    num_batches = 0\\n    for batch in test_dataset:\\n        input_ids = batch['input_ids']\\n        labels = batch['labels']\\n        loss = model(input_ids, labels=labels).loss\\n        test_loss += loss.numpy()\\n        num_batches += 1\\n    perplexity = math.exp(test_loss / num_batches)\\n    return perplexity\\n\\nperplexity = calculate_perplexity(model, tokenizer, test_dataset)\\nprint(perplexity)\\n```\\n\\nBy following these steps, you can evaluate the performance of a large language model and identify areas for improvement.\",\n",
      "                \"split content\": \"Slide 1:\\n\\nTitle: Evaluating a Large Language Model\\n\\n- Introduction\\n- Importance of evaluating language models\\n- Metrics used to evaluate language models: accuracy, perplexity, F1-score\\n\\nSlide 2:\\n\\nTitle: Preparing Your Data\\n\\n- The first step in evaluating a language model\\n- Cleaning the dataset\\n- Removing irrelevant information\\n- Converting the data into a format the model can understand\\n\\nSlide 3:\\n\\nTitle: Training Your Model\\n\\n- The second step in evaluating a language model\\n- Different architectures can be used: RNNs, LSTMs, Transformers\\n- Example using a Transformer model with code snippet\\n- Compiling and fitting the model\\n\\nSlide 4:\\n\\nTitle: Testing Your Model\\n\\n- The third step in evaluating a language model\\n- Evaluating the performance on a test set\\n- Example using generate_text() function with code snippet\\n\\nSlide 5:\\n\\nTitle: Evaluating Your Model\\n\\n- The fourth and final step in evaluating a language model\\n- Different metrics used to evaluate the model: accuracy, perplexity, F1-score\\n- Example of calculating perplexity with code snippet\\n\\nSlide 6:\\n\\nTitle: Conclusion\\n\\n- Importance of evaluating a language model to identify areas for improvement\\n- Recap of the evaluation steps: prepare the data, train the model, test the model, evaluate the model\\n- Potential for further research and development in the field of language models.\"\n",
      "            },\n",
      "            \"4\": {\n",
      "                \"title\": \"Interpreting the Results\",\n",
      "                \"full content\": \"Interpreting the Results:\\n\\n- After running the code or executing a script, the results are displayed on the screen or saved to a file.\\n- It is important to carefully analyze and interpret the results to understand what the code has done and whether it has achieved the intended outcome.\\n- Some common ways to interpret the results include:\\n    - Observing and analyzing the output values or plots generated by the code.\\n    - Comparing the obtained results with the expected or desired outcome.\\n    - Checking for errors or warnings generated during the execution of the code.\\n    - Identifying potential issues or bugs that need to be fixed.\\n- For example, if we have written a program to add two numbers and the result obtained is not equal to the expected sum, we need to carefully go through the code and debug it to identify the issue.\\n- Similarly, if we have written a script to plot a graph and the plot generated doesn't look as we desired, we need to closely examine the code to check if we have passed the right parameters to the plot function and if there are any errors in the script.\\n- Another way to interpret the results is to analyze the statistical measures such as mean, standard deviation, correlation coefficient, etc. For instance, if we want to measure the correlation between two variables, we can use the correlation function in Python, as shown below:\\n\\nimport numpy as np\\nx = np.array([1, 2, 3, 4, 5])\\ny = np.array([2, 4, 6, 8, 10])\\ncorr_coef = np.corrcoef(x, y)[0, 1]\\nprint(\\\"Correlation Coefficient: \\\", corr_coef)\\n\\nIn this example, the correlation coefficient between 'x' and 'y' is 1, which means that the two variables are perfectly positively correlated.\\n- In summary, interpreting the results is a crucial step in programming that involves carefully analyzing and understanding the code output, checking for errors or issues, and comparing the obtained results with the expected outcome.\",\n",
      "                \"split content\": \"Slide 1:\\n- Title: Interpreting the Results\\n- After running the code or executing a script, the results are displayed on the screen or saved to a file.\\n- It is important to carefully analyze and interpret the results to understand what the code has done and whether it has achieved the intended outcome.\\n\\nSlide 2:\\n- Title: Ways to Interpret Results\\n- Some common ways to interpret the results include:\\n    - Observing and analyzing the output values or plots generated by the code.\\n    - Comparing the obtained results with the expected or desired outcome.\\n    - Checking for errors or warnings generated during the execution of the code.\\n    - Identifying potential issues or bugs that need to be fixed.\\n\\nSlide 3:\\n- Title: Debugging Example\\n- For example, if we have written a program to add two numbers and the result obtained is not equal to the expected sum, we need to carefully go through the code and debug it to identify the issue.\\n\\nSlide 4:\\n- Title: Plotting Example\\n- Similarly, if we have written a script to plot a graph and the plot generated doesn't look as we desired, we need to closely examine the code to check if we have passed the right parameters to the plot function and if there are any errors in the script.\\n\\nSlide 5:\\n- Title: Analyzing Statistical Measures\\n- Another way to interpret the results is to analyze the statistical measures such as mean, standard deviation, correlation coefficient, etc.\\n\\nSlide 6:\\n- Title: Correlation Coefficient Example\\n- For instance, if we want to measure the correlation between two variables, we can use the correlation function in Python, as shown below:\\n    import numpy as np\\n    x = np.array([1, 2, 3, 4, 5])\\n    y = np.array([2, 4, 6, 8, 10])\\n    corr_coef = np.corrcoef(x, y)[0, 1]\\n    print(\\\"Correlation Coefficient: \\\", corr_coef)\\n- In this example, the correlation coefficient between 'x' and 'y' is 1, which means that the two variables are perfectly positively correlated.\\n\\nSlide 7:\\n- Title: Summary\\n- In summary, interpreting the results is a crucial step in programming that involves carefully analyzing and understanding the code output, checking for errors or issues, and comparing the obtained results with the expected outcome.\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "# Define the content_prompt template\n",
    "content_prompt = PromptTemplate(\n",
    "    input_variables=[\"subtopic\"],\n",
    "    template=\"Give me a detailed description of {subtopic}. Write in bulleted form and make sure to use code examples so that any beginner-level student can understand.\"\n",
    ")\n",
    "\n",
    "\n",
    "slide_prompt = PromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    template=\"distribute the content in {content} to N number of slides intelligently. the contents in the slides must be extracted from {content} and please don't write heading on what can we include in the slides , just take the content from {content} and distribute them.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the LLMChain instance for splitting content\n",
    "split_chain = LLMChain(llm=llm, prompt = slide_prompt )\n",
    "\n",
    "# Create the LLMChain instance for generating content\n",
    "content_chain = LLMChain(llm=llm, prompt=content_prompt)\n",
    "\n",
    "data = json.loads(module_output)\n",
    "\n",
    "\n",
    "# Function to generate content for a subtopic\n",
    "def generate_content(subtopic):\n",
    "    # Generate the full content using content_chain\n",
    "    full_content = content_chain.predict(subtopic=subtopic)\n",
    "    # Split the full content into multiple parts\n",
    "    splitted_content = split_chain.predict(content = full_content)\n",
    "    return {\n",
    "        \"title\": subtopic,\n",
    "        \"full content\": full_content,\n",
    "        \"split content\": splitted_content\n",
    "\n",
    "    }\n",
    "\n",
    "# Generate contents for each subtopic in every module using ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    for module in data.values():\n",
    "        subtopics = module[\"Subtopics\"]\n",
    "        # Create a list of futures for content generation\n",
    "        futures = [executor.submit(generate_content, subtopic) for subtopic in subtopics.values()]\n",
    "        # Retrieve the results from the futures and update the subtopics\n",
    "        for future, (subtopic_number, _) in zip(futures, subtopics.items()):\n",
    "            content = future.result()\n",
    "            subtopics[subtopic_number] = content\n",
    "\n",
    "# Save the updated module_output to a JSON file\n",
    "with open(\"module_output.json\", \"w\") as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "# Print the updated module_output\n",
    "print(json.dumps(data, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
